{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Specific Filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> Each keyword search might need a different final treatment: (1) erasing specific papers, (2) assigning publication years missing </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('GWABM.csv')\n",
    "df[['TI','DI']]\n",
    "# Main DB cleanup. Here we erase the 8 papers with No GW or No Human Behavior.\n",
    "df_eliminated = df.loc[df['DI'] == \"10.1016/J.ECOLMODEL.2019.04.015\"]\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.3390/W10091184\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1007/S40710-015-0082-6\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1080/14649357.2015.1045015\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1007/S10980-014-0145-5\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1016/J.ENVSOFT.2013.03.001\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1659/MRD-JOURNAL-D-11-00038.S1\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1007/S10113-011-0238-5\"])\n",
    " \n",
    "\n",
    "# Now we generate a list with all indexes to erase and erase the papers\n",
    "list_to_erase = df_eliminated.index.tolist()\n",
    "\n",
    "print(\"Initial Database shape is:\", df.shape)\n",
    "df = df.drop(list_to_erase)\n",
    "df = df.reset_index(drop=True)\n",
    "print(\"Final Database shape is:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we erase specific papers by adding them to the Pandas Dataframe \"df_eliminated\"\n",
    "df_eliminated = df.loc[df['DI'] == \"10.1016/S1251-8050(99)80113-X\"]\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1046/J.1365-3091.2001.00419.X\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1016/J.GEOMORPH.2015.12.018\"])\n",
    "\n",
    "# Now we generate a list with all indexes to erase and erase the papers\n",
    "list_to_erase = df_eliminated.index.tolist()\n",
    "\n",
    "print(\"Initial Database shape is:\", df.shape)\n",
    "df = df.drop(list_to_erase)\n",
    "df = df.reset_index(drop=True)\n",
    "print(\"Final Database shape is:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we save all NaN values on the Publication Year field to a list (var: \"PY_nulls_list\") \n",
    "PY_nulls_list = df.loc[df['PY'].isnull()].index\n",
    "PY_nulls_list\n",
    "# We now print the associated Titles of all these documents without publication year for manual editing\n",
    "for i in PY_nulls_list:\n",
    "    print(df.loc[i, ['PY', 'TI']])\n",
    "\n",
    "# For the manual edit we create a list of years to feed back to the Database and make corresponding changes\n",
    "PY_new = [2019,...]\n",
    "j = 0\n",
    "for i in PY_nulls_list:\n",
    "    df.loc[i, 'PY'] = PY_new[j]\n",
    "    j+=1\n",
    "\n",
    "# Finally we print a revision list\n",
    "for i in PY_nulls_list:\n",
    "    print(df.loc[i, ['PY', 'TI']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Databases Combination**  \n",
    "Previously saved cleaned Databases can be combined here, with a further duplicates removal through Titles and DOI identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we drop a specific row, reset index, check it has been removed and save it to database\n",
    "GW_ABM.shape\n",
    "GW_ABM = GW_ABM.drop([31],axis=0)\n",
    "GW_ABM = GW_ABM.reset_index(drop=True)\n",
    "GW_ABM.shape\n",
    "e_path = \"Keyword Searches/Main DB (ABM & GW)/Database/Main DB (ABM & GW)2.csv\"\n",
    "GW_ABM.to_csv(e_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GW_ABM = pd.read_csv(\"Keyword Searches/Main DB (ABM & GW)/Database/Main DB (ABM & GW).csv\")\n",
    "SH = pd.read_csv(\"Keyword Searches/SH/Database/SH.csv\")\n",
    "TC = pd.read_csv(\"Keyword Searches/TC/Database/TC.csv\")\n",
    "PM = pd.read_csv(\"Keyword Searches/PM/Database/PM.csv\")\n",
    "EM_2 = pd.read_csv(\"Keyword Searches/EM2/Database/EM2.csv\")\n",
    "GW_ABM.shape\n",
    "SH.shape\n",
    "TC.shape\n",
    "PM.shape\n",
    "EM_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [\"AU\",\"TI\",\"SO\",\"JI\",\"AB\",\"DE\",\"ID\",\"LA\",\"DT\",\"DT2\",\"TC\",\"CR\",\"C1\",\"DI\",\"AR\",\"FU\",\"SN\",\"PN\",\"PP\",\"PU\",\"VL\",\"PY\",\n",
    "        \"RP\",\"DB\",\"AU_UN\",\"AU1_UN\",\"AU_UN_NR\",\"SR_FULL\",\"SR\",\"CR_AU\",\"CR_SO\",\"AU_CO\",\"AU1_CO\", \"BN\"]\n",
    "\n",
    "# First we create an empty dataframe and paste all databases on it\n",
    "db_combined = pd.DataFrame(columns = list) \n",
    "db_combined = db_combined.append(GW_ABM, ignore_index=True)\n",
    "db_combined = db_combined.append(SH, ignore_index=True)\n",
    "db_combined = db_combined.append(TC, ignore_index=True)\n",
    "db_combined = db_combined.append(PM, ignore_index = True)\n",
    "#db_combined = db_combined.append(EM, ignore_index = True)\n",
    "db_combined = db_combined.append(EM_2, ignore_index = True)\n",
    "print(\"Database combined dimensions are: \", db_combined.shape)\n",
    "\n",
    "db_combined = db_combined.drop_duplicates(subset = \"TI\")\n",
    "print(\"After removing duplicates from titles, database dimensions are: \", db_combined.shape)\n",
    "#print(db_combined.isna().sum())\n",
    "\n",
    "db_combined_nulls = db_combined[db_combined['DI'].isnull()]\n",
    "print(\"missing DI values: \", db_combined_nulls.shape)\n",
    "db_combined_not_nulls = db_combined[db_combined['DI'].notnull()]\n",
    "print(\"actual DI: \", db_combined_not_nulls.shape)\n",
    "\n",
    "# Then we drop duplicates through the DI columns, by filtering only through non-null values\n",
    "filt = db_combined_not_nulls.drop_duplicates(subset = \"DI\")\n",
    "filt = filt.append(db_combined_nulls, ignore_index = True)\n",
    "print(\"removing duplicates through DI leaves: \", filt.shape)\n",
    "\n",
    "db_combined = filt.copy()\n",
    "print(\"Cleaned combined database dimensions are: \", db_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = db_combined.copy()\n",
    "print('Merged Database dimensions are: ', merged.shape)\n",
    "e_path = \"Keyword Searches/Merged DB (GW-ABM,SH,PM,TC,EM)/Database/Merged DB (GW-ABM,SH,PM,TC,EM).csv\"\n",
    "merged.to_csv(e_path, index = False)\n",
    "#check = pd.read_csv(e_path)\n",
    "#check"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "SALAR",
   "language": "python",
   "name": "salar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
