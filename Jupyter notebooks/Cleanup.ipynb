{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code has two purposes:  \n",
    "1) Import an existing CSV file from bibliometrix, drop duplicates by the DOI identifier (and not loose nan values), and then clean the following columns:  \n",
    "- Author keywords: All existing keywords are exported to a \"csv\" to cluster different spellings of each important term. This then feeds back to the database through a separate excel file that must be compile by the user \"Checklist_DE.xlsx\" following the stablished format (old value - new value).\n",
    "- Sources: All existing sources are exported to a \"csv\" to cluster different spellings of each important term. This then feeds back to the database through a separate excel file that must be compile by the user \"CheckList_SO.xlsx\" following the stablished format (old value - new value).\n",
    "- First author country:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **I - Import**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **I.I - New Database from Bibliometrix**\n",
    "We import the Dataframe (df) compiled through Bibliometrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Insert API key from Plotly:  2NexotbGiaTPZ5x7CNWa\n",
      "Insert user name from Plotly:  mlcanales\n"
     ]
    }
   ],
   "source": [
    "api = input(\"Insert API key from Plotly: \")\n",
    "user = input(\"Insert user name from Plotly: \")\n",
    "# 2NexotbGiaTPZ5x7CNWa; mlcanales\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "import pandas as pd\n",
    "import os\n",
    "import plotly\n",
    "import chart_studio\n",
    "import plotly.io as pio\n",
    "import chart_studio.plotly as py \n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "from plotly.offline import plot\n",
    "from glob import glob\n",
    "import shutil\n",
    "chart_studio.tools.set_credentials_file(username=user, api_key=api)\n",
    "\n",
    "\n",
    "# To get all results printed: (or \"last_expr\" to only last expression from cell)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macadmin/Google Drive/Shared Folders/GW-ABM — Review/Bibliometric analysis/Jupyter notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/macadmin/Google Drive/Shared Folders/GW-ABM — Review/Bibliometric analysis\n"
     ]
    }
   ],
   "source": [
    "# We start from the Jupyter notebooks folder\n",
    "os.getcwd()\n",
    "# Move back to main path: Bibliometric analysis\n",
    "%cd ..\n",
    "main_path = os.getcwd() + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Merged_dataframe_2.csv')\n",
    "df.shape\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1055, 37)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **I.II - Updated Database from Bilbiometrix**  \n",
    "Here we import the updated database through bibliometrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "df2 = pd.read_csv(\"Updated_Database.csv\")\n",
    "#print(df2.isna().sum())\n",
    "#df2\n",
    "df2 = df2.drop('Unnamed: 0', 1)\n",
    "#df2\n",
    "df = df2.copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **II - Oficial Cleanup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop duplicates by column \"DI\" from the .csv (due to nan values, this step must be done in python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database dimensions are:  (1055, 37)\n",
      "Indexes removed from the dataframe are:  [545, 562, 568, 595, 596, 620, 634, 669, 670, 681, 715, 741, 820, 844, 894, 1038, 1047, 1050]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     TI    INVESTIGATING THE IMPACT OF RIVER FLOODS...\n",
       "1     TI    ASSESSING VULNERABILITY TO FLOODING USIN...\n",
       "2     TI    MONITORING OF FLOOD EMBANKMENTS WITH THE...\n",
       "3     TI    HYDROSEDIMENTARY CONNECTIVITY IN A SMALL...\n",
       "4     TI    REASONING ABOUT RIVER BASINS WAWO REVISI...\n",
       "5     TI    AGENT BASED MODELLING FORWATER RESOURCE ...\n",
       "6     TI    MEASURING THE IMPACT OF LINEAR LANDSCAPE...\n",
       "7     TI    SIMULATING STAKEHOLDER BEHAVIOR IN A MAR...\n",
       "8     TI    AGENTBASED SIMULATION APPROACH FOR MANAG...\n",
       "9     TI    ASSESSING GROUNDWATER POLICY WITH COUPLE...\n",
       "10    TI    COMPLEMENTARY METHODS TO PLAN PEDESTRIAN...\n",
       "11    TI    UBIQUITOUS MULTIAGENT ENVIRONMENTAL HAZA...\n",
       "12    TI    MODELLING WITH STAKEHOLDERS TO INTEGRATE...\n",
       "13    TI    LAND MARKET INTERACTIONS BETWEEN HETEROG...\n",
       "14    TI    THE USE OF AGENT BASED MODELLING TECHIQU...\n",
       "15    TI    APPLICATION OF AN EULERIANLAGRANGLANAGEN...\n",
       "16    TI    MODELLING OF THE CONSTRUCTION OF THE RHO...\n",
       "17    TI    A NEW APPROACH FOR MODELLING SEDIMENT DE...\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database dimensions are:  (1037, 37)\n"
     ]
    }
   ],
   "source": [
    "print(\"Database dimensions are: \", df.shape)\n",
    "\n",
    "# Series of \"DI\" data from dataframe\n",
    "s = df.loc[:,['DI']]\n",
    "\n",
    "# Dropping NaN values (so they don't get dropped in duplicated detection)\n",
    "s = s.dropna()\n",
    "\n",
    "# Leaving only duplicated values\n",
    "s = s.duplicated()\n",
    "s = s[s == True] \n",
    "\n",
    "# Geting list of index values to drop in original dataframe\n",
    "s_index = s.index.tolist()\n",
    "print(\"Indexes removed from the dataframe are: \", s_index)\n",
    "list2 = []\n",
    "for element in s_index:\n",
    "    list2.append(df.loc[element,['TI']])\n",
    "serie = pd.Series(list2)\n",
    "serie\n",
    "\n",
    "# Editing original dataframe and reseting index (drop=true avoids adding the old index as column)\n",
    "df = df.drop(s_index)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(\"Database dimensions are: \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Database Path selection (GWABM - SH - TC - PM - EM - EM2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select from GWABM - SH - TC - PM - EM - EM2 - Floods:  Floods\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cleaning files to:  Cleaning Files Floods/\n",
      "Comparison Files are used on each subsection\n"
     ]
    }
   ],
   "source": [
    "# Here we create the variable db_path used to direct the analysis to each specific component\n",
    "db = input(\"Select from GWABM - SH - TC - PM - EM - EM2 - Floods: \")\n",
    "db_path = \"Cleaning Files \" + db + \"/\"                   \n",
    "print(\"Writing cleaning files to: \", db_path)\n",
    "print(\"Comparison Files are used on each subsection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Author Keywords (DE) &#9745;**\n",
    "**DE column extracted from dataframe to single csv file with non-duplicated keywords. CheckList generated by hand from this in excel. Dataframe is modified directly reading the CheckList**  \n",
    "The main idea is to detect different spellings of some keywords, which are stated in the CheckList file by reviewing the \"csv\" file generated first here. This CheckList works as a guide to re-edit the dataframe in-situ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ammount of Author Keywords detected are:  (3879,)\n",
      "Dataframe generated from author keywords with dimensions:  (3879, 1)\n",
      "Dataframe with co-occurrences now has the following dimensions:  (2529, 2)\n"
     ]
    }
   ],
   "source": [
    "# First we clean the Sources column from \"/&\" and \"&\"\" characters\n",
    "df['DE'] = df['DE'].str.replace('\\&', 'AND', regex=False)\n",
    "df['DE'] = df['DE'].str.replace('&', 'AND', regex=False)\n",
    "\n",
    "# We extract a list with keywords from the dataframe\n",
    "df_list = df['DE'].tolist()\n",
    "# NaN values are erased\n",
    "df_list_cleaned = [str(x) for x in df_list if str(x) != 'nan']\n",
    "# The list is splitted since there are several keywords in each row of the dataframe\n",
    "full_list = []\n",
    "for element in df_list_cleaned:\n",
    "    full_list.extend(element.split(';'))\n",
    "\n",
    "# We save the keywords to series (s1) and remove spacings in words\n",
    "s1 = pd.Series(full_list)\n",
    "print(\"Ammount of Author Keywords detected are: \", s1.shape)\n",
    "s1 = s1.str.strip()\n",
    "\n",
    "# We generate a dataframe from this column and then add a new column with co-occurrences counting\n",
    "dataframe = pd.DataFrame(columns = ['DE'])\n",
    "dataframe['DE'] = s1\n",
    "print(\"Dataframe generated from author keywords with dimensions: \", dataframe.shape)\n",
    "dataframe = dataframe.sort_values(by=['DE'])\n",
    "dataframe = dataframe.reset_index(drop=True)\n",
    "dataframe_test = pd.DataFrame({'Count' : dataframe.groupby( [\"DE\"] ).size()}).reset_index()\n",
    "print(\"Dataframe with co-occurrences now has the following dimensions: \", dataframe_test.shape)\n",
    "#dataframe_test\n",
    "\n",
    "# The final author keywords are exported to \".csv\" to detect and cluster similar ones and compile a CheckList by hand\n",
    "e_path = db_path + 'Spelling_Check_DE.xlsx'\n",
    "dataframe_test.to_excel(e_path, header=True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check \"Spelling_Check_DE.xlsx\" file and write over \"CheckList_DE.xlsx\" the new keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the \"CheckList_DE.xlsx\" has been created (with the 2 columns format: old value - new value), it is used to edit the dataframe\n",
    "e_path = \"Comparison Files/CheckList_DE.xlsx\"\n",
    "check_list = pd.read_excel(e_path)\n",
    "# First we get rid of NaN values replacing them by an empty string \"\"\n",
    "df['DE'] = df['DE'].fillna(\"\")\n",
    "# Each row of the selected column (\"DE\") is compiled into list1, cleaned in list2, and repaste to dataframe\n",
    "list1 = []\n",
    "list2 = []\n",
    "for index, row in df.iterrows():\n",
    "    list1.extend(row['DE'].split(';'))\n",
    "    for element in list1:\n",
    "        list2.append(element.strip())\n",
    "    # Here list2 is a list with the first row of the dataframe cleaned from spaces and ready for comparison:\n",
    "    i = 0\n",
    "    for element in list2:\n",
    "        for index, column in check_list.iterrows():\n",
    "            if element == column[0]:\n",
    "                list2[i] = column[1]\n",
    "                break\n",
    "        i = i + 1\n",
    "    #Here list2 has been transformed with specific \"terms\" listed in check_list\n",
    "    #Now we must add it back to original dataframe\n",
    "    df.replace(row['DE'], \"; \".join(list2), inplace = True)\n",
    "    #And finally clean all values for next row in dataframe\n",
    "    list1 = []\n",
    "    list2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check we print the Author Keywords column and visually check it has been edited properly according to the Excel file designed and imported in previous cell\n",
    "print(df.isna().sum())\n",
    "df['DE']\n",
    "# To check a particular cell value:\n",
    "#df.loc[[45], ['DE']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sources (SO) &#9745;**\n",
    "**SO column extracted from dataframe to single csv file with non-duplicated keywords. CheckList generated by hand from this in excel. Dataframe is modified directly reading the CheckList**  \n",
    "The main idea is to extract the Sources stored in the dataframe into a \"csv\" which does not stores \"nan\" or duplicates. Then, manually similar spellings of the same source can be detected and pasted in the CheckList file which acts as a guide for the dataframe edition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we clean the Sources column from \"/&\" and \"&\"\" characters\n",
    "df['SO'] = df['SO'].str.replace('\\&', 'AND', regex=False)\n",
    "df['SO'] = df['SO'].str.replace('&', 'AND', regex=False)\n",
    "\n",
    "# We extract a list with sources from the dataframe\n",
    "df_list = df['SO'].tolist()\n",
    "# NaN values are erased\n",
    "df_list_cleaned = [str(x) for x in df_list if str(x) != 'nan']\n",
    "\n",
    "# We save the sources to series (s1) and remove spacings in words\n",
    "s1 = pd.Series(df_list_cleaned)\n",
    "print(\"Ammount of sources detected are: \", s1.shape)\n",
    "s1 = s1.str.strip()\n",
    "\n",
    "# We generate a dataframe from this column and then add a new column with co-occurrences counting\n",
    "dataframe = pd.DataFrame(columns = ['SO'])\n",
    "dataframe['SO'] = s1\n",
    "\n",
    "print(\"Dataframe generated from Sources with dimensions: \", dataframe.shape)\n",
    "dataframe = dataframe.sort_values(by=['SO'])\n",
    "dataframe = dataframe.reset_index(drop=True)\n",
    "dataframe_test = pd.DataFrame({'Count' : dataframe.groupby( [\"SO\"] ).size()}).reset_index()\n",
    "print(\"Dataframe with co-occurrences of sources now has the following dimensions: \", dataframe_test.shape)\n",
    "#dataframe_test\n",
    "\n",
    "# The final sources are exported to \".csv\" to detect and cluster similar ones and compile a CheckList by hand\n",
    "e_path = db_path + 'Spelling_Check_SO.xlsx'\n",
    "dataframe_test.to_excel(e_path, header=True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check the \"Spelling_Check_SO.xlsx\" file for sources to re-write (mispelled or different spellings) and write \"CheckList_SO.xlsx\" in the format explicited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_path = 'Comparison Files/CheckList_SO.xlsx'\n",
    "check_list = pd.read_excel(e_path) #Follow same 2 column format: old value - new value\n",
    "\n",
    "# First we get rid of NaN values replacing them by an empty string \"\"\n",
    "df['SO'] = df['SO'].fillna(\"\")\n",
    "# Each row of the selected column (\"SO\") is compiled into list1, cleaned in list2, and repaste to dataframe\n",
    "list1 = []\n",
    "list2 = []\n",
    "for index, row in df.iterrows():\n",
    "    list1.extend(row['SO'].split(';'))\n",
    "    for element in list1:\n",
    "        list2.append(element.strip())\n",
    "    # Here list2 is a list with the first row of the dataframe cleaned from spaces and ready for comparison:\n",
    "    i = 0\n",
    "    for element in list2:\n",
    "        for index, column in check_list.iterrows():\n",
    "            if element == column[0]:\n",
    "                list2[i] = column[1]\n",
    "                break\n",
    "        i = i + 1\n",
    "    #Here list2 has been transformed with specific \"terms\" listed in check_list\n",
    "    #Now we must add it back to original dataframe\n",
    "    df.replace(row['SO'], \", \".join(list2), inplace = True)\n",
    "    #And finally clean all values for next row in dataframe\n",
    "    list1 = []\n",
    "    list2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check we print the Author Keywords column and visually check it has been edited properly according to the Excel file designed and imported in previous cell\n",
    "df['SO']\n",
    "# To check a particular cell value:\n",
    "#df.loc[[45], ['SO']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Author Countries (AU1_CO) &#9745;**\n",
    "**AU1_CO column extracted from dataframe to single csv file with non-duplicated keywords. CheckList generated by hand from this in excel. Dataframe is modified directly reading the CheckList**  \n",
    "The main idea is to extract the Country of Affiliation of each first author stored in the dataframe, and detect all \"nan\" values, so that they can then be updated into the dataframe. Also, it is important to check whether there is different spellings of the same country. The Co-authors country of affiliation is not checked here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a list of Country of Affiliation of each first author (AU1_CO) and co-author\n",
    "# Here, the idea is to check if all countries under AU1_CO are written correctily (i.e. USA vs United States) to group them correctly.\n",
    "df_sliced = df[['DI', 'TI', 'AU', 'AU1_CO', 'AU_CO']]\n",
    "\n",
    "# First for spelling issues we check the different countries listed in column AU1_CO\n",
    "df_not_null = df_sliced.dropna(subset = ['AU1_CO'])\n",
    "df_not_null = df_not_null.sort_values(by =['AU1_CO'])\n",
    "df_not_null = df_not_null.drop_duplicates(subset = \"AU1_CO\")\n",
    "\n",
    "e_path = db_path + 'Spelling_Check_AU1_CO.xlsx'\n",
    "df_not_null.to_excel(e_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, to add the missing countries, we will have to detect them, saved them to excel and add them manually by looking at WOS or Scopus\n",
    "df_null_1 = df_sliced[df[\"AU1_CO\"].isnull()]\n",
    "df_null_1 = df_null_1.reindex(columns=['DI', 'TI', 'AU', 'AU1_CO', 'AU_CO', 'AU1_CO_NEW'], fill_value = \"\")\n",
    "\n",
    "e_path = db_path + 'NaN_Check_AU1_CO.xlsx'\n",
    "df_null_1.to_excel(e_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check the \"Spelling_Check_AU1_CO.xlsx\" file for mispelled elements and the \"NaN_Check_AU1_CO.xlsx\" for NaN values. Replaces for both are added to \"CheckList_AU1_CO.xlsx\" in the \"AU1_CO_NEW\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After manually adding in the column \"AU1_CO_NEW\" the respective countries needed, we save the excel as \"NaN_Check_AU1_CO_IN.xlsx\" to finally edit \"df\"\n",
    "e_path = db_path + 'CheckList_AU1_CO.xlsx'\n",
    "check_list = pd.read_excel(e_path) #Follow same format: old value - new value\n",
    "\n",
    "# First we get rid of NaN values replacing them by an empty string \"\" in df\n",
    "df['AU1_CO'] = df['AU1_CO'].fillna(\"\")\n",
    "# For each row of the selected column (\"AU1_CO\") of df, we compare the NaN check excel through the DI column and edit its values.\n",
    "for index1, row in df.iterrows():\n",
    "    for index2, column in check_list.iterrows():\n",
    "        if column[0] == index1:\n",
    "            df.loc[index1,'AU1_CO'] = column[6]\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"if column[1] == row['DI']:\n",
    "            df.loc[index1,'AU1_CO'] = column[6]\n",
    "            #df.replace(row[34], column[6], inplace = True)\n",
    "            break\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AU1_CO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **III.I - GW_ABM Database Specific Filtering &#9745;**\n",
    "**Here we manually erase those documents not needed for our analysis**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# We first identify by visualizing the dataframe, those papers to erase and add them to df_eliminated by filtering thorugh the DI column\n",
    "df_eliminated = df.loc[df['DI'] == \"10.1016/S1251-8050(99)80113-X\"]\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1046/J.1365-3091.2001.00419.X\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1016/J.GEOMORPH.2015.12.018\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1016/J.SNB.2017.04.026\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1016/J.WSE.2018.07.006\"])\n",
    "df_eliminated = df_eliminated.append(df.iloc[df[df.TI == 'REGIONALSUBREGIONAL 2D3D APPROACH TO MODEL GROUNDWATER FLOW AT SANDIA NATIONAL LABORATORIES ALBUQUERQUE NEW MEXICO USA'].index])\n",
    "df_eliminated = df_eliminated.append(df.iloc[df[df.TI == 'EVOLUTIONARY MODEL OF COAL MINE WATER HAZARDS BASED ON MULTIAGENT SIMULATION'].index])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1180/CLAYMIN.2013.048.2.05\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1007/S10653-013-9531-1\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1371/JOURNAL.PONE.0150626\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.1016/J.APGEOCHEM.2016.07.005\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.12011/1000-6788(2017)12-3289-08\"])\n",
    "df_eliminated = df_eliminated.append(df.loc[df['DI'] == \"10.11975/J.ISSN.1002-6819.2019.11.023\"])\n",
    "df_eliminated = df_eliminated.append(df.iloc[df[df.TI == 'PURPOSE PROCESSES PARTNERSHIPS AND PRODUCTS FOUR PS TO ADVANCE PARTICIPATORY SOCIOENVIRONMENTAL MODELING'].index])\n",
    "list_to_erase = df_eliminated.index.tolist()\n",
    "df.shape\n",
    "df = df.drop(list_to_erase)\n",
    "df = df.reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Now, after assessing through zotero, we eliminate all documents with no groundwater modelled or human behaviour\n",
    "# Always look for the DI or the Title in the dataframe, since there is where python will look for\n",
    "# df[['TI', 'DI']]\n",
    "df_eliminated2 = df.loc[df['DI'] == \"10.1080/1747423X.2018.1499828\"]\n",
    "df_eliminated2 = df_eliminated2.append(df.loc[df['DI'] == \"10.1016/J.JHYDROL.2012.02.032\"])\n",
    "df_eliminated2 = df_eliminated2.append(df.loc[df['DI'] == \"10.1016/J.JCLEPRO.2018.12.065\"])\n",
    "df_eliminated2 = df_eliminated2.append(df.loc[df['DI'] == \"10.1016/J.AGEE.2004.08.007\"])\n",
    "df_eliminated2 = df_eliminated2.append(df.loc[df['DI'] == \"10.1002/2014WR016825\"])\n",
    "df_eliminated2 = df_eliminated2.append(df.loc[df['DI'] == \"10.5751/ES-08642-210323\"])\n",
    "df_eliminated2 = df_eliminated2.append(df.iloc[df[df.TI == 'MODELING LEARNING AND PLANNING TOGETHER AN APPLICATION OF PARTICIPATORY AGENTBASED MODELING TO ENVIRONMENTAL PLANNING'].index])\n",
    "df_eliminated2 = df_eliminated2.append(df.loc[df['DI'] == \"10.1002/EAP.1627\"])\n",
    "list_to_erase = df_eliminated2.index.tolist()\n",
    "df.shape\n",
    "df = df.drop(list_to_erase)\n",
    "df = df.reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **III.II - SH Database Specific Filtering**\n",
    "**Here we manually assign Publication Years missing in the database**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we detect NaN values on PY\n",
    "PY_nulls_list = SH.loc[SH['PY'].isnull()].index\n",
    "PY_nulls_list\n",
    "for i in PY_nulls_list:\n",
    "    print(SH.loc[i, ['PY', 'TI']])\n",
    "\n",
    "# Changes\n",
    "PY_new = [2019, 2019, 2019]\n",
    "j = 0\n",
    "for i in PY_nulls_list:\n",
    "    SH.loc[i, 'PY'] = PY_new[j]\n",
    "    j+=1\n",
    "\n",
    "# Revision\n",
    "for i in PY_nulls_list:\n",
    "    print(SH.loc[i, ['PY', 'TI']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.loc[df['SO']==\"AMBIO\"]\n",
    "df.loc[256, ['TI']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **III.III - TC Database Specific Filtering**\n",
    "**Here we manually assign Publication Years missing in the database**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we detect NaN values on PY\n",
    "PY_nulls_list = TC.loc[TC['PY'].isnull()].index\n",
    "PY_nulls_list\n",
    "for i in PY_nulls_list:\n",
    "    print(TC.loc[i, ['PY', 'TI']])\n",
    "\n",
    "# Changes\n",
    "PY_new = [2019]\n",
    "j = 0\n",
    "for i in PY_nulls_list:\n",
    "    TC.loc[i, 'PY'] = PY_new[j]\n",
    "    j+=1\n",
    "\n",
    "# Revision\n",
    "for i in PY_nulls_list:\n",
    "    print(TC.loc[i, ['PY', 'TI']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **III.IV - PM Database Specific Filtering**\n",
    "**Here we manually assign Publication Years missing in the database**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we detect NaN values on PY\n",
    "PY_nulls_list = PM.loc[PM['PY'].isnull()].index\n",
    "PY_nulls_list\n",
    "for i in PY_nulls_list:\n",
    "    print(PM.loc[i, ['PY', 'TI']])\n",
    "\n",
    "# Changes\n",
    "PY_new = [2019, 2019, 2019]\n",
    "j = 0\n",
    "for i in PY_nulls_list:\n",
    "    PM.loc[i, 'PY'] = PY_new[j]\n",
    "    j+=1\n",
    "\n",
    "# Revision\n",
    "for i in PY_nulls_list:\n",
    "    print(PM.loc[i, ['PY', 'TI']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **III.V - EM Database Specific Filtering**\n",
    "**Here we manually erase those documents not needed for our analysis**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we detect NaN values\n",
    "PY_nulls_list = EM.loc[EM['PY'].isnull()].index\n",
    "PY_nulls_list\n",
    "for i in PY_nulls_list:\n",
    "    print(EM.loc[i, ['PY', 'TI']])\n",
    "\n",
    "# Changes:\n",
    "PY_new = [2019]\n",
    "j = 0\n",
    "for i in PY_nulls_list:\n",
    "    EM.loc[i, 'PY'] = PY_new[j]\n",
    "    j+=1\n",
    "# Revision\n",
    "for i in PY_nulls_list:\n",
    "    print(EM.loc[i, ['PY', 'TI']])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **III.VI - EM_2 Database Specific Filtering**\n",
    "**Here we manually erase those documents not needed for our analysis**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we detect NaN values\n",
    "PY_nulls_list = EM_2.loc[EM_2['PY'].isnull()].index\n",
    "PY_nulls_list\n",
    "for i in PY_nulls_list:\n",
    "    print(EM_2.loc[i, ['PY', 'TI']])\n",
    "\n",
    "# Changes:\n",
    "PY_new = [2019, 2019, 2019, 2019, 2019, 2019]\n",
    "j = 0\n",
    "for i in PY_nulls_list:\n",
    "    EM_2.loc[i, 'PY'] = PY_new[j]\n",
    "    j+=1\n",
    "# Revision\n",
    "for i in PY_nulls_list:\n",
    "    print(EM_2.loc[i, ['PY', 'TI']])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Databases**  \n",
    "**Each database iteration is saved over these temporary variables as dataframes and csv files, ready for using in bibliometrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GW_ABM DATABASE &#9745;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#GW_ABM = df.copy()\n",
    "print('GWABM Database dimensions are: ', GW_ABM.shape)\n",
    "e_path = \"Cleaning Files GWABM/GWABM_DB.csv\"\n",
    "GW_ABM.to_csv(e_path, index = False)\n",
    "#check = pd.read_csv(e_path)\n",
    "#check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SOCIOHYDROLOGY DATABASE &#9745;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SH = df.copy()\n",
    "print('SH Database dimensions are: ', SH.shape)\n",
    "e_path = \"Cleaning Files SH/SH_DB.csv\"\n",
    "SH.to_csv(e_path, index = False)\n",
    "#check = pd.read_csv(e_path)\n",
    "#check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TELECOUPLING DATABASE &#9745;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#TC = df.copy()\n",
    "print('TC Database dimensions are: ', TC.shape)\n",
    "e_path = \"Cleaning Files TC/TC_DB.csv\"\n",
    "TC.to_csv(e_path, index = False)\n",
    "#check = pd.read_csv(e_path)\n",
    "#check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PARTICIPATORY MODELLING DATABASE &#9745;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#PM = df.copy()\n",
    "print('PM Database dimensions are: ', PM.shape)\n",
    "e_path = \"Cleaning Files PM/PM_DB.csv\"\n",
    "PM.to_csv(e_path, index = False)\n",
    "#check = pd.read_csv(e_path)\n",
    "#check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EXPLORATORY MODELLING DATABASE &#9745;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#EM = df.copy()\n",
    "print('EM Database dimensions are: ', EM.shape)\n",
    "e_path = \"Cleaning Files EM/EM_DB.csv\"\n",
    "EM.to_csv(e_path, index = False)\n",
    "#check = pd.read_csv(e_path)\n",
    "#check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **EXPLORATORY MODELLING WITH DEEP UNCERTAINTY AND ROBUST DECISION MAKING &#9745;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#EM_2 = df.copy()\n",
    "print('EM_2 (with \"robust\" and \"deep uncertainty\") Database dimensions are: ', EM_2.shape)\n",
    "e_path = \"Cleaning Files EM2/EM2_DB.csv\"\n",
    "EM_2.to_csv(e_path, index = False)\n",
    "#check = pd.read_csv(e_path)\n",
    "#check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Other Databases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sociohydrology and Groundwater &#9745;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "SH_GW = df.copy()\n",
    "print('SH_GW Database dimensions are: ', SH_GW.shape)\n",
    "e_path = \"GW Related Frameworks/SH_GW_DB.csv\"\n",
    "SH_GW.to_csv(e_path, index = False)\n",
    "#check = pd.read_csv(e_path)\n",
    "#check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Specific filtering: Here we erase documents before 2012\n",
    "SH_GW[\"PY\"].sort_values()\n",
    "#SH_GW.drop([46,52,45,44], inplace=True)\n",
    "SH_GW.reset_index(drop=True)\n",
    "SH_GW[\"PY\"].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Telecoupling and Groundwater &#9745;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "TC_GW = df.copy()\n",
    "print('TC_GW Database dimensions are: ', TC_GW.shape)\n",
    "e_path = \"GW Related Frameworks/TC_GW_DB.csv\"\n",
    "TC_GW.to_csv(e_path, index = False)\n",
    "#check = pd.read_csv(e_path)\n",
    "#check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Participatory Modelling and Groundwater &#9745;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "PM_GW = df.copy()\n",
    "print('PM_GW Database dimensions are: ', PM_GW.shape)\n",
    "e_path = \"GW Related Frameworks/PM_GW_DB.csv\"\n",
    "PM_GW.to_csv(e_path, index = False)\n",
    "#check = pd.read_csv(e_path)\n",
    "#check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exploratory Modelling and Groundwater &#9745;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EM_GW = df.copy()\n",
    "print('EM_GW (with \"robust\" and \"deep uncertainty\") Database dimensions are: ', EM_GW.shape)\n",
    "e_path = \"GW Related Frameworks/EM_GW_DB.csv\"\n",
    "EM_GW.to_csv(e_path, index = False)\n",
    "#check = pd.read_csv(e_path)\n",
    "#check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Flooding and ABM &#9745;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FL_ABM Database dimensions are:  (1037, 37)\n"
     ]
    }
   ],
   "source": [
    "FL_ABM = df.copy()\n",
    "print('FL_ABM Database dimensions are: ', FL_ABM.shape)\n",
    "e_path = \"Cleaning Files Floods/FL_ABM_DB.csv\"\n",
    "FL_ABM.to_csv(e_path, index = False)\n",
    "#check = pd.read_csv(e_path)\n",
    "#check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Databases Combination**  \n",
    "**Cleaned Databases can be combined here before a final export to bibliometrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(267, 34)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(128, 34)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(349, 36)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(786, 36)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GW_ABM = pd.read_csv(\"Keyword Searches/Main DB (ABM & GW)/Database/Main DB (ABM & GW).csv\")\n",
    "SH = pd.read_csv(\"Keyword Searches/SH/Database/SH.csv\")\n",
    "TC = pd.read_csv(\"Keyword Searches/TC/Database/TC.csv\")\n",
    "PM = pd.read_csv(\"Keyword Searches/PM/Database/PM.csv\")\n",
    "EM_2 = pd.read_csv(\"Keyword Searches/EM2/Database/EM2.csv\")\n",
    "GW_ABM.shape\n",
    "SH.shape\n",
    "TC.shape\n",
    "PM.shape\n",
    "EM_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database combined dimensions are:  (1570, 37)\n",
      "After removing duplicates from titles, database dimensions are:  (1558, 37)\n",
      "missing DI values:  (46, 37)\n",
      "actual DI:  (1512, 37)\n",
      "removing duplicates through DI leaves:  (1556, 37)\n",
      "Cleaned combined database dimensions are:  (1556, 37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macadmin/anaconda3/envs/SALAR/lib/python3.8/site-packages/pandas/core/frame.py:7134: FutureWarning:\n",
      "\n",
      "Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list = [\"AU\",\"TI\",\"SO\",\"JI\",\"AB\",\"DE\",\"ID\",\"LA\",\"DT\",\"DT2\",\"TC\",\"CR\",\"C1\",\"DI\",\"AR\",\"FU\",\"SN\",\"PN\",\"PP\",\"PU\",\"VL\",\"PY\",\n",
    "        \"RP\",\"DB\",\"AU_UN\",\"AU1_UN\",\"AU_UN_NR\",\"SR_FULL\",\"SR\",\"CR_AU\",\"CR_SO\",\"AU_CO\",\"AU1_CO\", \"BN\"]\n",
    "\n",
    "# First we create an empty dataframe and paste all databases on it\n",
    "db_combined = pd.DataFrame(columns = list) \n",
    "db_combined = db_combined.append(GW_ABM, ignore_index=True)\n",
    "db_combined = db_combined.append(SH, ignore_index=True)\n",
    "db_combined = db_combined.append(TC, ignore_index=True)\n",
    "db_combined = db_combined.append(PM, ignore_index = True)\n",
    "#db_combined = db_combined.append(EM, ignore_index = True)\n",
    "db_combined = db_combined.append(EM_2, ignore_index = True)\n",
    "print(\"Database combined dimensions are: \", db_combined.shape)\n",
    "\n",
    "db_combined = db_combined.drop_duplicates(subset = \"TI\")\n",
    "print(\"After removing duplicates from titles, database dimensions are: \", db_combined.shape)\n",
    "#print(db_combined.isna().sum())\n",
    "\n",
    "db_combined_nulls = db_combined[db_combined['DI'].isnull()]\n",
    "print(\"missing DI values: \", db_combined_nulls.shape)\n",
    "db_combined_not_nulls = db_combined[db_combined['DI'].notnull()]\n",
    "print(\"actual DI: \", db_combined_not_nulls.shape)\n",
    "\n",
    "# Then we drop duplicates through the DI columns, by filtering only through non-null values\n",
    "filt = db_combined_not_nulls.drop_duplicates(subset = \"DI\")\n",
    "filt = filt.append(db_combined_nulls, ignore_index = True)\n",
    "print(\"removing duplicates through DI leaves: \", filt.shape)\n",
    "\n",
    "db_combined = filt.copy()\n",
    "print(\"Cleaned combined database dimensions are: \", db_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Database dimensions are:  (1556, 37)\n"
     ]
    }
   ],
   "source": [
    "merged = db_combined.copy()\n",
    "print('Merged Database dimensions are: ', merged.shape)\n",
    "e_path = \"Keyword Searches/Merged DB (GW-ABM,SH,PM,TC,EM)/Database/Merged DB (GW-ABM,SH,PM,TC,EM).csv\"\n",
    "merged.to_csv(e_path, index = False)\n",
    "#check = pd.read_csv(e_path)\n",
    "#check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final treatment and export to bibliometrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we obtain the tags that must not be consider while reupdating the database to bibliometrix\n",
    "df_tags = pd.Series(df.columns.values)\n",
    "list = [\"AB\", \"AR\", \"AU\", \"AU_UN\", \"AU_UN_NR\", \"AU1_UN\", \"BN\", \"C1\", \"CR\", \"DB\", \"DE\", \"DI\", \"DT\", \n",
    "                     \"DT2\", \"FU\", \"ID\", \"JI\", \"LA\", \"PN\", \"PP\", \"PU\", \"PY\", \"RP\", \"SN\", \"SO\", \"SR\", \"SR_FULL\", \n",
    "                     \"TC\", \"TI\", \"VL\", \"CR_AU\", \"CR_SO\", \"AU_CO\", \"AU1_CO\"]\n",
    "official_wos_tags = pd.Series(list)\n",
    "tags_concat = pd.concat([df_tags, official_wos_tags])\n",
    "tags_concat = tags_concat.sort_index()\n",
    "tags_concat = tags_concat.drop_duplicates(keep = False)\n",
    "print(\"To reupdate database to bibliometrix, the next columns must not be consider: \")\n",
    "tags_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final export to bibliometrix\n",
    "db_combined.to_csv('Database.csv', index = False)\n",
    "asd = pd.read_csv(\"Database.csv\")\n",
    "#print(asd.isna().sum())\n",
    "asd.shape\n",
    "#asd['DE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **IV - Export dataframe DOI identifier to Zotero**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we transform the column \"DI\" (DOI Identifier) to a text file in order to import it to zotero through the \"add item\n",
    "# by identifier option\". For this, manually copy the files in the text file to this option (magic wand) in zotero.\n",
    "# NaN files in DOI must be added manually to Zotero\n",
    "list_zotero = GW_ABM['DI'].tolist()\n",
    "list_zotero = [x for x in list_zotero if str(x) != 'nan']\n",
    "with open('Zotero/list_zotero.txt', 'w') as file:\n",
    "    for listitem in list_zotero:\n",
    "        file.write('%s\\n' % listitem)\n",
    "\n",
    "#df[['DI', 'TI']]\n",
    "#df2 = df.loc[df['DI'] == \"NaN\"]\n",
    "#df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GW_ABM[['TI',\"DI\"]]\n",
    "#pd.isna(GW_ABM[\"DI\"].iloc[i])\n",
    "df2 = GW_ABM.loc[GW_ABM['DI'] == \"NaN\"]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(\"/Users/macadmin/Google Drive/GW-ABM — Review/Bibliometric Analysis/\")\n",
    "df = pd.read_csv('FL_ABM_DB.csv')\n",
    "pd.set_option('display.max_columns', 100)  # To show all columns on results\n",
    "pd.set_option('display.max_rows', 5000) \n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df[\"DE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SALAR",
   "language": "python",
   "name": "salar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
