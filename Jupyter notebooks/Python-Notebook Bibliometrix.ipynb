{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook documents the cleaning procedure of a Bibliographic Database (Excel File) generated through the [Bibliometrix R-package](https://www.bibliometrix.org) (developed by Massimo Aria and Corrado Cuccurullo) in the **\"R-Notebook Bibliometrix\"**. Then it displays interactive outputs and visualisations.  \n",
    "The complete **Github repository** is available in GitHub at [Research Quilting](https://github.com/sim4action/Research-Quilting).   \n",
    "\n",
    "The Overall Workflow for developing a new search relies on two notebooks: \"Python-Notebook Bibliometrix\" and \"R-Notebook Bibliometrix\". It goes as follows:\n",
    "**1.** \n",
    "2. \n",
    "3. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false"
   },
   "source": [
    "# **Set-up**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DEPENDENCIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import shutil                        # to move files\n",
    "from IPython.display import Image    # to display images in code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To graph with Plotly offline:\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To graph with Plotly online:\n",
    "api = input(\"Insert API key from Plotly: \")\n",
    "user = input(\"Insert user name from Plotly: \")\n",
    "# 2NexotbGiaTPZ5x7CNWa; mlcanales\n",
    "import chart_studio\n",
    "import plotly.io as pio\n",
    "import chart_studio.plotly as py \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "chart_studio.tools.set_credentials_file(username=user, api_key=api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get all results printed: (or \"last_expr\" to only last expression from cell)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # To show all columns on results\n",
    "pd.set_option('display.max_rows', None)     # To show all rows on results\n",
    "pd.set_option('display.max_colwidth', -1) # To display all length of each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **WORKING DIRECTORY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We set the working directory in \"Research Quilting\" folder:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/macadmin/Desktop/Projects/Research Quilting\n",
      "main_path is:  /Users/macadmin/Desktop/Projects/Research Quilting/\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "main_path = os.getcwd() + \"/\"\n",
    "print(\"main_path is: \",main_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We also edit the \"Working directory for R.txt\" file to set the working directory of the \"R-Notebook Bibliometrix\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/macadmin/Desktop/Projects/Research Quilting/Jupyter notebooks/Working directory for R.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"Working directory for R.txt\",\"w+\")\n",
    "file.write(main_path)\n",
    "file.write('\\n')\n",
    "file.close()\n",
    "shutil.move(main_path + 'Working directory for R.txt',main_path+'Jupyter notebooks/'+'Working directory for R.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **WORKFLOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename= \"Workflow.png\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKING FOLDERS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For each keyword search we make a unique folder that centralizes all of its files. The following code generates the needed folders for the keyword search. This cell must only be executed to create the folders (i.e. it is a new clean search). If the folders already exist, only relative paths are needed in the rest of the notebook, for which the following cell is not necessary (skip to next one).**  \n",
    "- **Cleaning Files**: Contains the excel files working as dictionaries in the cleaning procedure ahead.\n",
    "- **Database**: Contains the final cleaned Database (after cleaning procedure).\n",
    "- **Input Data**: Contains the input BibTeX files downloaded from search engines.\n",
    "- **Outputs**: Contains Figures (screenshots) and Network (.net) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose name of Keyword Search/Database:  mitochondrial dynamics\n"
     ]
    }
   ],
   "source": [
    "# Folder for the specific Keyword search\n",
    "db = input(\"Choose name of Keyword Search/Database: \")\n",
    "os.mkdir(db)\n",
    "kw_from = main_path + db\n",
    "kw_to = main_path + \"Keyword Searches/\" + db + \"/\"\n",
    "shutil.move(kw_from,kw_to)\n",
    "\n",
    "# Folder for Input Data\n",
    "input_folder = \"Input Data\"\n",
    "os.mkdir(input_folder)\n",
    "data_from = main_path + input_folder\n",
    "data_to = kw_to + input_folder + \"/\"\n",
    "shutil.move(data_from,data_to)\n",
    "\n",
    "# Folder for Cleaning files\n",
    "cleaning_folder = \"Cleaning Files\"\n",
    "os.mkdir(cleaning_folder)\n",
    "kw_cleaning_from = main_path + cleaning_folder\n",
    "kw_cleaning_to = kw_to + cleaning_folder + \"/\"\n",
    "shutil.move(kw_cleaning_from,kw_cleaning_to)\n",
    "\n",
    "# Folder for Database cleaned\n",
    "database_folder = \"Database\"\n",
    "os.mkdir(database_folder)\n",
    "database_from = main_path + database_folder\n",
    "database_to = kw_to + database_folder + \"/\"\n",
    "shutil.move(database_from,database_to)\n",
    "\n",
    "# Main Folder Outputs\n",
    "output_folder = \"Outputs\"\n",
    "os.mkdir(output_folder)\n",
    "output_from = main_path + output_folder\n",
    "output_to = kw_to + output_folder + \"/\"\n",
    "shutil.move(output_from,output_to)\n",
    "\n",
    "# Subfolders Outputs\n",
    "sub_outputs_list = [\"Figures\", \"Networks\"]\n",
    "for element in sub_outputs_list:\n",
    "    os.mkdir(element)\n",
    "    source_path = main_path + element\n",
    "    destination_path1 = output_to + element + \"/\"\n",
    "    shutil.move(source_path,destination_path1)\n",
    "    if element == \"Networks\":\n",
    "        net_list = [\"Co-citations\",\"Collaborations\",\"Bibliographic Coupling\",\"Co-occurrences\"]\n",
    "        for element in net_list:\n",
    "            os.mkdir(element)\n",
    "            source_path = main_path + element\n",
    "            destination_path2 = destination_path1 + element\n",
    "            shutil.move(source_path,destination_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELATIVE PATHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we first set the appropiate and needed (notebook references to these paths) relative paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = input(\"Choose name of Keyword Search (i.e. Database): \")\n",
    "kw_from = main_path + db\n",
    "kw_to = main_path + \"Keyword Searches/\" + db + \"/\"\n",
    "kw_cleaning_from = main_path + \"Cleaning Files\"\n",
    "kw_cleaning_to = kw_to + \"Cleaning Files/\"\n",
    "database_from = main_path + \"Database\"\n",
    "database_to = kw_to + \"Database/\"\n",
    "output_from = main_path + \"Outputs\"\n",
    "output_to = kw_to + \"Outputs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "# **Importing Bibliometrix CSV for Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We now read the Database as a Pandas Dataframe (variable: \"df\"), and print its dimensions and the number of missing elements on each field. Is important to notice that in this code we rely on the \"Titles\" field of the Database to be complete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the Database are:  (2158, 35)\n",
      "\"Titles\" tag field of Database has:  0 missing elements (must be 0 for cleaning procedure)\n",
      "Unnamed: 0       0\n",
      "AU               0\n",
      "TI               0\n",
      "SO               0\n",
      "JI               0\n",
      "AB             114\n",
      "DE             540\n",
      "ID             103\n",
      "LA               0\n",
      "DT               0\n",
      "DT2              0\n",
      "TC               0\n",
      "CR              69\n",
      "C1              25\n",
      "DI              38\n",
      "AR            1460\n",
      "BE            2145\n",
      "FU            1398\n",
      "SN              64\n",
      "PN             688\n",
      "PP             747\n",
      "PU               5\n",
      "VL              45\n",
      "PY              23\n",
      "RP              86\n",
      "DB               0\n",
      "AU_UN           33\n",
      "AU1_UN           0\n",
      "AU_UN_NR      2158\n",
      "SR_FULL          0\n",
      "SR               0\n",
      "CR_AU           69\n",
      "CR_SO           69\n",
      "AU_CO           38\n",
      "AU1_CO         945\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('db_r_to_py.csv')\n",
    "print('Dimensions of the Database are: ', df.shape)\n",
    "print('\"Titles\" tag field of Database has: ', df['TI'].isna().sum(), 'missing elements (must be 0 for cleaning procedure)')\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Duplicates Removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we aim to drop any duplicates remaining in the Database by using the DOI identifier of papers (column \"DI\"). We print at the beginning and the end the Database dimensions, and also the papers that have been removed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Initial Dimensions are:  (2158, 35)\n",
      "Indexes removed from the dataframe are:  [539, 540, 543, 567, 582, 585, 626, 633, 638, 640, 682, 683, 684, 711, 712, 713, 714, 716, 727, 730, 731, 744, 780, 790, 792, 793, 794, 823, 824, 825, 837, 859, 918, 1093, 1110]\n",
      "Database Final Dimensions are (whitout duplicates):  (2123, 35)\n"
     ]
    }
   ],
   "source": [
    "print(\"Database Initial Dimensions are: \", df.shape)\n",
    "\n",
    "# We first generate a Pandas Series from the \"DI\" (DOI identifier) column of the Database\n",
    "s = df.loc[:,['DI']]\n",
    "\n",
    "# We drop missing values from this series (i.e. missing DOIs), so they don't get dropped in the duplicated detection\n",
    "s = s.dropna()\n",
    "\n",
    "# Now we keep only the duplicated rows (papers)\n",
    "s = s.duplicated()\n",
    "s = s[s == True] \n",
    "\n",
    "# We get a list of index values from this series, and print the values to erase\n",
    "s_index = s.index.tolist()\n",
    "print(\"Indexes removed from the dataframe are: \", s_index)\n",
    "list2 = []\n",
    "for element in s_index:\n",
    "    list2.append(df.loc[element,['TI']])\n",
    "serie = pd.Series(list2)\n",
    "serie\n",
    "\n",
    "# Finally, we use the index from this series to remove duplicated papers in the original database, and then we reset the index.\n",
    "df = df.drop(s_index)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(\"Database Final Dimensions are (whitout duplicates): \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false",
    "toc-hr-collapsed": false
   },
   "source": [
    "## **Field Tags Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> Each following subsection follows the same two-step approach to: (1) clean misspelled elements and (2) cluster different spellings of the same concepts (e.g. \"agent based\" and \"agent-based\"). In all cases, the process is done in two consecutive steps: First, we compile Excel files (e.g. \"Spelling_Check_DE\") for the user to manually detect terms to change or to cluster. Second, we write the desired changes in another Excel file (e.g. \"Checklist_DE\") that follows an old-value and new-value format, and works as a dictionay that automatically feeds and edits the original Database. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Author Keywords (DE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> The first part of the code compiles an Excel file (\"Spelling_Check_DE\") with all detected author keywords in the Database for the manual analysis. Desired changes must be written in the Excel file (\"Checklist_DE\"). The second part of the code feeds from this Dictionary (Excel file) to automatically edit the original Database </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ammount of total (with duplicates) Author Keywords detected are:  (9027,)\n",
      "Ammount of non-duplicated Author Keywords detected are:  (3987, 2)\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: First we clean the Author Keywords column of the Database from characters such as: /&; &; \"\" (quotes); etc...\n",
    "df['DE'] = df['DE'].str.replace('\\&', 'AND', regex=False)\n",
    "df['DE'] = df['DE'].str.replace('&', 'AND', regex=False)\n",
    "df[\"DE\"] = df['DE'].str.replace('\"','', regex=False)\n",
    "\n",
    "\n",
    "# STEP 2: Second, we count the ammount of Author Keywords detected in the Database\n",
    "# For this, we first generate a list (var: df_list_cleaned) containing all keywords on the Database (excluding \"NaN\" or missing values)\n",
    "df_list = df['DE'].tolist()\n",
    "df_list_cleaned = [str(x) for x in df_list if str(x) != 'nan']\n",
    "\n",
    "# Since each papers has several keywords on it, now we split this list to have one keyword on each row (var: full_list)\n",
    "full_list = []\n",
    "for element in df_list_cleaned:\n",
    "    full_list.extend(element.split(';'))\n",
    "\n",
    "# We now transform this list to a Pandas Series (var: s1), remove spacings in words and print the ammount of keywords detected\n",
    "s1 = pd.Series(full_list)\n",
    "s1 = s1.str.strip()\n",
    "print(\"Ammount of total (with duplicates) Author Keywords detected are: \", s1.shape)\n",
    "\n",
    "\n",
    "# STEP 3: Third, we generate a dataframe with the cleaned author keywords and the individual counting of each (var: dataframe_test)\n",
    "# For this, we first transform this series (var: s1) into a Pandas Dataframe (var: dataframe)\n",
    "dataframe = pd.DataFrame(columns = ['DE'])\n",
    "dataframe['DE'] = s1\n",
    "\n",
    "# Now we sort all keywords by name\n",
    "dataframe = dataframe.sort_values(by=['DE'])\n",
    "dataframe = dataframe.reset_index(drop=True)\n",
    "\n",
    "# Finally we generate a new Pandas dataframe (var: dataframe_test) with both the author keywords and the co-occurrences counting\n",
    "dataframe_test = pd.DataFrame({'Count' : dataframe.groupby( [\"DE\"] ).size()}).reset_index()\n",
    "print(\"Ammount of non-duplicated Author Keywords detected are: \", dataframe_test.shape)\n",
    "\n",
    "\n",
    "# STEP 4: The final step consists of a manual (human) check of similar keywords by using the generated Excel file (\"Spelling_Check_DE\"),\n",
    "# and compile the Dictionary through the Excel file \"Checklist_DE\"\n",
    "e_path = kw_cleaning_to + 'Spelling_Check_DE.xlsx'\n",
    "dataframe_test.to_excel(e_path, header=True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now open the \"Spelling_Check_DE.xlsx\" file and write over the \"CheckList_DE.xlsx\" file the new desired keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the \"CheckList_DE.xlsx\" has been created, it is used to edit the Database. \n",
    "# For this, we first read the file (var: check_list):\n",
    "e_path = main_path + \"Comparison Files/CheckList_DE.xlsx\"\n",
    "check_list = pd.read_excel(e_path)\n",
    "\n",
    "# Then we get rid of missing values replacing them by an empty string \"\"\n",
    "df['DE'] = df['DE'].fillna(\"\")\n",
    "\n",
    "# Finally we use this dictionary to edit the original Database \n",
    "# For this, the keywords of each paper in the Database are stored in a list (var: list2), then properly transformed according to the \n",
    "# Dictionary, and finally pasted back to the Database.\n",
    "list1 = []\n",
    "list2 = []\n",
    "for index, row in df.iterrows():\n",
    "    list1.extend(row['DE'].split(';'))\n",
    "    for element in list1:\n",
    "        list2.append(element.strip())\n",
    "    # Here list2 is a list with the first row of the dataframe cleaned from spaces and ready for comparison:\n",
    "    i = 0\n",
    "    for element in list2:\n",
    "        for index, column in check_list.iterrows():\n",
    "            if element == column[0]:\n",
    "                list2[i] = column[1]\n",
    "                break\n",
    "        i = i + 1\n",
    "    # Here list2 has been transformed according to the dictionary\n",
    "    # Now we finally paste this list back to the Database\n",
    "    df.replace(row['DE'], \"; \".join(list2), inplace = True)\n",
    "    # Here we clean all lists for the next iteration (row) in the Database\n",
    "    list1 = []\n",
    "    list2 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sources (SO)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> The first part of the code compiles an Excel file (\"Spelling_Check_SO\") with all detected sources in the Database for the manual analysis. Desired changes must be written in the Excel file (\"Checklist_SO\"). The second part of the code feeds from this Dictionary (Excel file) to automatically edit the original Database </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ammount of Total sources (with duplicates) detected is:  (396,)\n",
      "Ammount of non-duplicated Sources detected is:  (266, 2)\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: First we clean the Sources column of the Database from characters such as: /&; &; \"\" (quotes); etc...\n",
    "df['SO'] = df['SO'].str.replace('\\&', 'AND', regex=False)\n",
    "df['SO'] = df['SO'].str.replace('&', 'AND', regex=False)\n",
    "df['SO'] = df['SO'].str.replace('\"','', regex=False)\n",
    "\n",
    "\n",
    "# STEP 2: Second, we exdetected in the Database\n",
    "# For this, we first generate a list (var: df_list_cleaned) containing all sources on the Database (excluding \"NaN\" or missing values)\n",
    "df_list = df['SO'].tolist()\n",
    "df_list_cleaned = [str(x) for x in df_list if str(x) != 'nan']\n",
    "\n",
    "# We now transform this list to a Pandas Series (var: s1), remove spacings in words and print the ammount of sources detected\n",
    "s1 = pd.Series(df_list_cleaned)\n",
    "s1 = s1.str.strip()\n",
    "print(\"Ammount of Total sources (with duplicates) detected is: \", s1.shape)\n",
    "\n",
    "\n",
    "# STEP 3: Third, we generate a dataframe with sources and the individual counting of each (var: dataframe_test)\n",
    "# For this, we first transform this series (var: s1) into a Pandas Dataframe (var: dataframe)\n",
    "dataframe = pd.DataFrame(columns = ['SO'])\n",
    "dataframe['SO'] = s1\n",
    "\n",
    "# Now we sort all sources by name\n",
    "dataframe = dataframe.sort_values(by=['SO'])\n",
    "dataframe = dataframe.reset_index(drop=True)\n",
    "\n",
    "# Finally we generate a new Pandas dataframe (var: dataframe_test) with both sources and the co-occurrences counting\n",
    "dataframe_test = pd.DataFrame({'Count' : dataframe.groupby( [\"SO\"] ).size()}).reset_index()\n",
    "print(\"Ammount of non-duplicated Sources detected is: \", dataframe_test.shape)\n",
    "\n",
    "\n",
    "# STEP 4: The final step consists of a manual (human) check of similar sources by using the generated Excel file (\"Spelling_Check_SO\"),\n",
    "# and compile the Dictionary through the Excel file \"Checklist_SO\"\n",
    "e_path = kw_cleaning_to + 'Spelling_Check_SO.xlsx'\n",
    "dataframe_test.to_excel(e_path, header=True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check the \"Spelling_Check_SO.xlsx\" file for sources to re-write (mispelled or different spellings) and write \"CheckList_SO.xlsx\" in the format explicited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the \"CheckList_SO.xlsx\" has been created, it is used to edit the Database. \n",
    "# For this, we first read the file (var: check_list):\n",
    "e_path = main_path + \"Comparison Files/CheckList_SO.xlsx\"\n",
    "check_list = pd.read_excel(e_path) \n",
    "\n",
    "# Then we get rid of missing values replacing them by an empty string \"\"\n",
    "df['SO'] = df['SO'].fillna(\"\")\n",
    "# Finally we use this dictionary to edit the original Database \n",
    "# For this, the keywords of each paper in the Database are stored in a list (var: list2), then properly transformed according to the \n",
    "# Dictionary, and finally pasted back to the Database.\n",
    "list1 = []\n",
    "list2 = []\n",
    "for index, row in df.iterrows():\n",
    "    list1.extend(row['SO'].split(';'))\n",
    "    for element in list1:\n",
    "        list2.append(element.strip())\n",
    "    # Here list2 is a list with the first row of the dataframe cleaned from spaces and ready for comparison:\n",
    "    i = 0\n",
    "    for element in list2:\n",
    "        for index, column in check_list.iterrows():\n",
    "            if element == column[0]:\n",
    "                list2[i] = column[1]\n",
    "                break\n",
    "        i = i + 1\n",
    "    # Here list2 has been transformed according to the dictionary\n",
    "    # Now we finally paste this list back to the Database\n",
    "    df.replace(row['SO'], \", \".join(list2), inplace = True)\n",
    "    # Here we clean all lists for the next iteration (row) in the Database\n",
    "    list1 = []\n",
    "    list2 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **First Author Countries (AU1_CO)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> The first part of the code compiles two Excel files that aim to check mispelled countries (\"Spelling_Check_AU1_CO\") and missing first author counties (\"NaN_Check_AU1_CO\"). Desired changes for the latter must be written in the Excel file (\"Checklist_AU1_CO\"). The second part of the code feeds from this Dictionary (Excel file) to automatically edit the original Database </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a list of Country of Affiliation of each first author (AU1_CO) and co-author\n",
    "# Here, the idea is to check if all countries under AU1_CO are written correctily (i.e. USA vs United States) to group them correctly.\n",
    "df_sliced = df[['DI', 'TI', 'AU', 'AU1_CO', 'AU_CO']]\n",
    "\n",
    "# First for spelling issues we check the different countries listed in column AU1_CO\n",
    "df_not_null = df_sliced.dropna(subset = ['AU1_CO'])\n",
    "df_not_null = df_not_null.sort_values(by =['AU1_CO'])\n",
    "df_not_null = df_not_null.drop_duplicates(subset = \"AU1_CO\")\n",
    "\n",
    "e_path = kw_cleaning_to + 'Spelling_Check_AU1_CO.xlsx'\n",
    "df_not_null.to_excel(e_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, to add the missing countries, we will have to detect them, saved them to excel and add them manually by looking at WOS or Scopus\n",
    "df_null_1 = df_sliced[df[\"AU1_CO\"].isnull()]\n",
    "df_null_1 = df_null_1.reindex(columns=['DI', 'TI', 'AU', 'AU1_CO', 'AU_CO', 'AU1_CO_NEW'], fill_value = \"\")\n",
    "\n",
    "e_path = kw_cleaning_to + 'NaN_Check_AU1_CO.xlsx'\n",
    "df_null_1.to_excel(e_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check the \"Spelling_Check_AU1_CO.xlsx\" file for mispelled elements and the \"NaN_Check_AU1_CO.xlsx\" for NaN values. Replaces for both are added to \"CheckList_AU1_CO.xlsx\" in the \"AU1_CO_NEW\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the \"CheckList_AU1_CO.xlsx\" has been created, it is used to edit the Database. \n",
    "# For this, we first read the file (var: check_list):\n",
    "e_path = main_path + \"Comparison Files/CheckList_SO.xlsx\"\n",
    "check_list = pd.read_excel(e_path) \n",
    "\n",
    "# First we get rid of NaN values replacing them by an empty string \"\" in the Database\n",
    "df['AU1_CO'] = df['AU1_CO'].fillna(\"\")\n",
    "\n",
    "# Finally, we edit the Database by comparing the DOI column and pasting new values\n",
    "for index1, row in df.iterrows():\n",
    "    for index2, column in check_list.iterrows():\n",
    "        if column[0] == index1:\n",
    "            df.loc[index1,'AU1_CO'] = column[6]\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Database Saving**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> The Database is saved to an Excel in \"Database\" folder and also on the main path to use directly on the Bibliometrix Notebook </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database dimensions are:  (396, 36)\n"
     ]
    }
   ],
   "source": [
    "database_path = database_to + db + \".csv\"\n",
    "X = df.copy()\n",
    "print('Database dimensions are: ', X.shape)\n",
    "X.to_csv(database_path, index = False)\n",
    "\n",
    "path_bibliometrix = main_path + \"db_py_to_r.csv\"\n",
    "X.to_csv(path_bibliometrix, index=False)\n",
    "#check = pd.read_csv(e_path)\n",
    "#check.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Database Export to Zotero**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> Here we transform the column \"DI\" (DOI Identifier of documents) to a text file (\"list_zotero.txt\" located in \"Zotero\" file), to upload the Database to zotero. For this, manually copy the files in the text file created, and paste them to the option \"add item by identifier\" in zotero.  \n",
    "Since all missing DOIs (not included in the text file) are not added to Zotero, we save them to (var:). </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ammount of documents in Database are:', df['DI'].size)\n",
    "print('Ammount of missing DOI in Database are:',df['DI'].isna().sum())\n",
    "print('Therefore, ammount of papers ready to be uploaded to Zotero are: ', df['DI'].size - df['DI'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_zotero = df['DI'].tolist()\n",
    "list_zotero = [x for x in list_zotero if str(x) != 'nan']\n",
    "zotero_path = output_to + \"Zotero_DOI_list.txt\"\n",
    "with open(zotero_path, 'w') as file:\n",
    "    for listitem in list_zotero:\n",
    "        file.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false"
   },
   "source": [
    "# **Outputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the database \".CSV\" file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(396, 36)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(database_path)\n",
    "df = pd.read_csv(\"Keyword Searches/Jupyter/Database/Jupyter.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false"
   },
   "source": [
    "## **Moving Networks files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we copy all bibliographic network files created through the R-Notebook Bibliometrix to the respective keyword search output folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_path = main_path + \"Temporal Outputs\" + \"/\"\n",
    "net_list = [\"Co-citations\",\"Collaborations\",\"Bibliographic Coupling\",\"Co-occurrences\"]\n",
    "for element in net_list:\n",
    "    source_path_1 = net_path + element + \"/\" + \"vosnetwork.net\"\n",
    "    destination_path_1 = output_to + \"Networks\" + \"/\" + element + \"/\" + \"vosnetwork.net\"\n",
    "    shutil.copy(source_path_1,destination_path_1)\n",
    "    source_path_2 = net_path + element + \"/\" + \"VOSviewer.jar\"\n",
    "    destination_path_2 = output_to + \"Networks\" + \"/\" + element + \"/\" + \"VOSviewer.jar\"\n",
    "    shutil.copy(source_path_2,destination_path_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false"
   },
   "source": [
    "## **Annual Scientific Production**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First we get the annual scientific production and erase year 2020 to avoid bias in graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annual Scientific Production\n",
    "ASP = pd.DataFrame({'Count' : df.groupby( [\"PY\"] ).size()})\n",
    "ASP = ASP.fillna(0)\n",
    "\n",
    "# We drop year 2020 to avoid bias in the graph\n",
    "ASP.drop(ASP.tail(1).index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we select Color Scale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color Selection\n",
    "cmap = plt.cm.get_cmap('tab20c')\n",
    "in_list = [0.1,0.3,0.5,0.7,0.8,0.9]\n",
    "colour_list=[]\n",
    "for element in in_list:\n",
    "    rgba = cmap(element)\n",
    "    rgba = tuple(int((255*x)) for x in rgba[0:3])\n",
    "    rgb = 'rgb'+str(rgba)\n",
    "    colour_list.append(rgb)\n",
    "colour_list\n",
    "\n",
    "cmap2 = plt.cm.get_cmap('viridis')\n",
    "colour_list2=[]\n",
    "for element in in_list:\n",
    "    rgba = cmap2(element)\n",
    "    rgba = tuple(int((255*x)) for x in rgba[0:3])\n",
    "    rgb = 'rgb'+str(rgba)\n",
    "    colour_list2.append(rgb)\n",
    "colour_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally we plot the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "                x = ASP.index.values,\n",
    "                y = ASP[\"Count\"],\n",
    "                mode = 'lines',\n",
    "                name = \"GW-ABM\",\n",
    "                #marker = dict(color = colour_list[0]),\n",
    "                text = ASP['Count'],\n",
    "                line = dict(color=colour_list[0], width=2.5))\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(width= 1000, height= 600,\n",
    "              xaxis= dict(title= '<b>Publication Year<b>',ticklen= 5,tick0=2003,dtick=1,zeroline=False,ticks = \"inside\", title_font=dict(size=18), showline=False, linewidth=1, linecolor='black'),\n",
    "              yaxis= dict(title= '<b>Number of Documents<b>',tick0 = 0,dtick = 1,ticklen= 5,zeroline=False, ticks = \"inside\", title_font=dict(size=18),showline=False, linewidth=1, linecolor='black'),\n",
    "              showlegend = True, legend=dict(orientation='h',yanchor='bottom',xanchor='center',y=1.002,x=0.5,font=dict(size=12)),\n",
    "              annotations=[dict(x=0.5,y=1.12,align=\"right\",valign=\"top\",text='<b>Legend<b>',font=dict(size=15),showarrow=False,xref=\"paper\",yref=\"paper\",xanchor=\"center\",yanchor=\"top\")],\n",
    "              template=\"plotly\")\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "# Merged = 1\n",
    "fig.add_annotation(\n",
    "    go.layout.Annotation(xref='paper',yref=\"paper\",x=1.01,y=0.88,xanchor='left', yanchor='middle', text='<b>CAGR</b>24%',font=dict(size=13,color=colour_list2[0]),showarrow=False))\n",
    "\n",
    "url = plot(fig, validate=False)\n",
    "url\n",
    "#\"plotly\", \"plotly_white\", \"plotly_dark\", \"ggplot2\", \"seaborn\", \"simple_white\", \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Compound Annual Growth Rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starting with the Annual Scientific Production, we can compute the ACGR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annual Scientific Production\n",
    "ASP = pd.DataFrame({'Count' : df.groupby( [\"PY\"] ).size()})\n",
    "ASP = ASP.fillna(0)\n",
    "ASP = ASP.cumsum()\n",
    "\n",
    "# We drop year 2020 to avoid bias in calculation\n",
    "ASP.drop(ASP.tail(1).index, inplace=True)\n",
    "\n",
    "# Computation of ACGR\n",
    "var = [\"Count\"]\n",
    "for element in var:\n",
    "    final_val = ASP[element].iloc[-1]\n",
    "    final_t = ASP[element].index[-1]\n",
    "    #print(final_val, final_t)\n",
    "    i = 0\n",
    "    initial_val = ASP[element].iloc[i]\n",
    "    #print(initial_val)\n",
    "    while initial_val == 0:\n",
    "        i+=1\n",
    "        initial_val = ASP[element].iloc[i]\n",
    "        #print(initial_val)\n",
    "    initial_t = ASP[element].index[i]\n",
    "    T = final_t-initial_t\n",
    "    print(\"Annual Compound Growth Rate for\",element,\"is: \", 100*((final_val/initial_val)**(1/(T+1))-1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false"
   },
   "source": [
    "## **Displaying Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First we set up the working directory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First we get a list with all the paths to images that will be displayed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for img_path in glob('*.png'):\n",
    "    images.append(img_path)\n",
    "images.sort()\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entire Screen Display**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First we load relevant libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we save the variables to plot and display them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List = []\n",
    "for i in images:\n",
    "    x = Image(filename = i, width=1000, height=600, retina =True)\n",
    "    List.append(x)    \n",
    "\n",
    "count = 0\n",
    "for element in List:\n",
    "    text = images[count]\n",
    "    text = '**' + text + '**'\n",
    "    display(Markdown(text))\n",
    "    count+=1\n",
    "    display(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Matplotlib Subplots Display**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import relevant libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we plot the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we get the appropiate format for each image \n",
    "images2 = []\n",
    "for i in images:\n",
    "    images2.append(mpimg.imread(i))\n",
    "\n",
    "# We generate the figure and select how many columns we are plotting (must be the same for each database or row)\n",
    "fig = plt.figure(figsize=(15,13))\n",
    "columns = 3\n",
    "for i, image in enumerate(images2):\n",
    "    plt.subplot(len(images2) / columns + 1, columns, i+1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(images[i])\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.15, wspace=0.01)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('myplot.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "SALAR",
   "language": "python",
   "name": "salar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
